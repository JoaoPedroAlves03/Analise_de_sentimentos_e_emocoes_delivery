{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**A ANÁLISE DE SENTIMENTOS DE MENSAGENS DO TWITTER ENVOLVENDO SERVIÇOS DE ENTREGA DE ALIMENTOS**\n",
        "\n",
        "- Autor/pesquisador: João Pedro Rodrigues Alves;\n",
        "- Instituição: Universidade Presbiteriana Mackenzie;\n",
        "- Curso: Ciência da Computação (FCI);\n",
        "- Orientador: Ivan Carlos Alcantara de Oliveira;\n",
        "- Apoio: PIBIC Mackenzie;\n",
        "\n",
        "Código fonte do modelo de aprendizagem de máquina para análise de sentimentos de clientes dos apps iFood, Rappi e Zé Delivery. O modelo foi alcançado a partir de um estudo aprofundado em um projeto de iniciação científica de mesmo título. Para utilização faça uploud da base de dado no diretório do notebook ou faça as alterações necessárias para alterar o modo de acessar os dados (atente-se ao nome do arquivo e aos nomes das colunas para evitar conflito de nomes com o código fornecido abaixo)."
      ],
      "metadata": {
        "id": "7JiQN9zginfn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXs46PGVLF8b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega atual diretório de trabalho deste notebook e armazena-o em wd\n",
        "wd = os.getcwd()\n",
        "print(wd)\n",
        "\n",
        "# Lista os arquivos na pasta dados\n",
        "if not os.path.exists(wd):\n",
        "  os.makedirs(wd)\n",
        "os.listdir(wd)"
      ],
      "metadata": {
        "id": "ob4lFNYWLma2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Abre o Workbook e mostra o nome das planilhas\n",
        "file = 'base_completa.xlsx'\n",
        "df_base = pd.ExcelFile(file)\n",
        "print(df_base.sheet_names)\n",
        "\n",
        "df_dados = df_base.parse('Sheet1')\n",
        "df_dados.info()"
      ],
      "metadata": {
        "id": "dbLGFDhoLos-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclui as colunas User e Date, as quais não serão relevantes na análise\n",
        "df_dados = df_dados.drop(columns=['User', 'Date'])\n",
        "df_dados.tail()"
      ],
      "metadata": {
        "id": "Q1wIueJtLrG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de geral de limpeza\n",
        "def limpar_texto(texto):\n",
        "    # Remove as citações de perfis (começado com @)\n",
        "    texto = re.sub(r'@[\\w\\d_]+', ' ', texto)\n",
        "\n",
        "    # Remove as hastags (começado com #)\n",
        "    texto = re.sub(r'#[\\w\\d_]+', '', texto)\n",
        "\n",
        "    # Remove a string RT isolada\n",
        "    texto = re.sub(r'\\bRT\\b', '', texto)\n",
        "\n",
        "    # Remove links de sites\n",
        "    texto = re.sub(r'http\\S+', '', texto)\n",
        "\n",
        "    # Converte todo o texto para minúsculo\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Remove pontuações, emojis e caracteres especiais\n",
        "    texto = re.sub(r'[^\\w\\s]|_', '', texto, flags=re.UNICODE)\n",
        "\n",
        "    # Remove acentos\n",
        "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "    # Remove números\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "\n",
        "    # Remove espaços extras\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    return texto"
      ],
      "metadata": {
        "id": "KIpF315RLyeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica a função de limpeza à coluna 'Tweet'\n",
        "df_dados['Clean Tweet'] = df_dados['Tweet'].apply(limpar_texto)\n",
        "df_dados.tail()"
      ],
      "metadata": {
        "id": "aYa1JFkOLzSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de tokenização e stemização\n",
        "def tk_stemmer_text(text):\n",
        "    tokens = word_tokenize(text.lower(), language='portuguese')\n",
        "    stemmer = RSLPStemmer()\n",
        "    normalized_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(normalized_tokens)"
      ],
      "metadata": {
        "id": "e8S9cin5L1-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download de pacotes necesários para aplicar a função\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "id": "MCrumrQxL7QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica a função de tokenização e stemização\n",
        "df_dados['Tweet Stemming'] = df_dados['Clean Tweet'].apply(tk_stemmer_text)\n",
        "df_dados.tail()"
      ],
      "metadata": {
        "id": "Tv3-WpwML8kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando entrada e saída do modelo\n",
        "X = df_dados['Tweet Stemming']\n",
        "y = df_dados['Feeling']\n",
        "\n",
        "# Criando a matriz de features usando TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(X)\n",
        "\n",
        "# Imprimindo a distribuição original das classes\n",
        "print('Distribuição original das classes:', Counter(y))\n",
        "\n",
        "# Divide os dados em conjunto de treino e teste\n",
        "X_train_smote, X_test, y_train_smote, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "print('Distribuição original das classes treino:', Counter(y_train_smote))\n",
        "print('Distribuição original das classes teste:', Counter(y_test))"
      ],
      "metadata": {
        "id": "zhE67IpnL-Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicação de SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_smote, y_train_smote)\n",
        "\n",
        "# Imprimindo a nova distribuição das classes após SMOTE\n",
        "print('Nova distribuição das classes após SMOTE:', Counter(y_train_smote))"
      ],
      "metadata": {
        "id": "atBY7wWpMKKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria o modelo de Naive Bayes\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Treina o modelo\n",
        "clf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Realiza a previsão\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Gera a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Imprime o relatório de classificação\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cm)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "Ssz3X16xMMZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
